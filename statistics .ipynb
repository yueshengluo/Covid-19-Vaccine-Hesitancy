{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "## Cleaning methods\n",
    "def clean_emoji(text):\n",
    "    text=emoji.demojize(text)\n",
    "    return text\n",
    "def remove_url(text):\n",
    "    text = ' '.join(re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\" \", text).split())\n",
    "    return text\n",
    "def remove_symbol(text):\n",
    "    text = str.replace(text,',',' ')\n",
    "    text = str.replace(text,'+',' ')\n",
    "    text = str.replace(text,'=',' ')\n",
    "    text = ' '.join(re.sub(\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\",\" \", text).split())\n",
    "    text = str.replace(text,\"'\",' ')\n",
    "    text = str.replace(text,'\"',' ')\n",
    "    text = str.replace(text,'!',' ')\n",
    "    text = str.replace(text,'^',' ')\n",
    "    text = str.replace(text,'(',' ')\n",
    "    text = str.replace(text,')',' ')\n",
    "    text = str.replace(text,'%',' ')\n",
    "    text = str.replace(text,'-',' ')\n",
    "    text = str.replace(text,'_',' ')\n",
    "    text = str.replace(text,'|',' ')\n",
    "    text = str.replace(text,'.',' ')\n",
    "    text = str.replace(text,':',' ')\n",
    "    return text    \n",
    "## this is the optional function we can use to remove @ and other. Might not use it if we need to analyze retweets\n",
    "def optional_rm(text):\n",
    "    all_words = text.split(' ')\n",
    "    remove_words = []\n",
    "    for each in all_words:\n",
    "        if each == ' ' or each == '':\n",
    "            continue\n",
    "        if each[0] == '#' or each[0] == '@':\n",
    "            remove_words.append(each)\n",
    "            if each[0] == '#':\n",
    "                hashtags = each.split('#')\n",
    "                for tag in hashtags:\n",
    "                    gl_hashtag.append(tag.lower())\n",
    "\n",
    "    for word in remove_words:\n",
    "        all_words.remove(word)\n",
    "    text = ' '.join(all_words)\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    new_text = []\n",
    "    for each in text:\n",
    "        if(str.find(each,'http') != -1):\n",
    "            continue\n",
    "        \n",
    "        if not each.isalnum():\n",
    "            continue\n",
    "        new_text.append(str.lower(each));\n",
    "    text = ' '.join(new_text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        if detect(text) == \"en\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return True\n",
    "        print(e)   \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True    \n",
    "def processing(text):\n",
    "    #text=clean_emoji(text)\n",
    "    #text=remove_url(text)\n",
    "    #text = remove_symbol(text)\n",
    "    text = optional_rm(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Import Data\n",
    "def read_json_to_dataframe(filename):\n",
    "    print(filename)\n",
    "    #f = open(filename,encoding = 'utf-16').read()\n",
    "    f1 = pd.read_json(filename, orient='records',lines=True,encoding = 'utf-16')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(r'C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\raw\\10-19-2020.json', orient = 'records',lines=True, encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10-19-2020.json', '10-20-2020.json', '10-21-2020.json', '10-22-2020.json', '10-23-2020.json', '10-24-2020.json', '10-25-2020.json']\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-19-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-20-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-21-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-22-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-23-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-24-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-25-2020.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(371940, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_in(path):\n",
    "    files = os.listdir(path)\n",
    "    all_dfs = []\n",
    "    print(files)\n",
    "    for i in files:\n",
    "        if 'json' in i:\n",
    "            each_df = read_json_to_dataframe(path+i)\n",
    "            all_dfs.append(each_df)\n",
    "            \n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "all_df = read_in('C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/')\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Get rid of the folded tweets\n",
    "test_df = all_df.copy()\n",
    "test_df=test_df[~test_df['og_tweet_txt'].str.endswith('\\u2026',na=True)]\n",
    "test_df=test_df[~((test_df['tweet_txt'].str.endswith('\\u2026',na=True)) & (test_df['og_tweet_txt'].str.strip() == \"\"))]\n",
    "# test_df=test_df.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open(\"./cleaned.pkl\", 'rb') as f:\n",
    "    clean_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retweet = test_df[test_df['og_tweet_time'].isna()].copy()\n",
    "retweet = test_df[test_df['og_tweet_time'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = pd.read_csv(r\"C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\lstm_pred\\LSTM_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_sample_6x100 = pd.read_excel(r\"C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\cluster_df_sample_6x100.xlsx\")\n",
    "cluster_df_sample_6x100 = cluster_df_sample_6x100[['combined_tweet_txt','true_class']]\n",
    "top_ambiguous = pd.read_excel(r\"C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\top_ambiguous.xlsx\")\n",
    "for_join = cluster_df_sample_6x100.append(top_ambiguous[['combined_tweet_txt','true_class']]).copy()\n",
    "for_join = for_join[for_join.true_class != 0]\n",
    "joined_df = for_join.set_index('combined_tweet_txt').join(lstm_pred.set_index('combined_tweet_txt'), how='inner')[['true_class','tweet_id']]\n",
    "joined_df = joined_df[~joined_df.index.duplicated(keep='first')]\n",
    "joined_df_2 = joined_df.set_index('tweet_id').join(all_df.set_index('tweet_id'), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "45\n",
      "['debates2020:', 'freedom', 'trump', 'trumpliesamericansdie', 'votebluedownballot', 'votehimout2020']\n",
      "6\n",
      "6\n",
      "77\n",
      "['british', 'coronavirus.”', 'newsrail:']\n",
      "3\n",
      "3\n",
      "174\n",
      "['astrazeneca', 'pharma', 'uk', 'th…', 'stopthemadness\\n', 'socialmedia', 'saynotothevaccine\\nvolunteer', 'riodejaneiro', 'pmlive', 'new:', 'bmj', 'healthmrx', 'health', 'globalbuzz', 'cyberattacks…', 'covidiots', 'coronavirus:', 'brazil', 'wakeupeverybody\\n']\n",
      "19\n",
      "21\n",
      "84\n",
      "['biharelections', 'biharelections2020', 'biofarmaterpercaya', 'bjp', 'bjp…', 'covid_19', 'covidー19', 'gatesfoundation', 'jio', 'leronlimab', 'manifesto', 'reliance', 'trumpvirus!', 'uk', 'worstpresidentinhistory…']\n",
      "15\n",
      "15\n",
      "14\n",
      "['meatballarmy']\n",
      "1\n",
      "1\n",
      "18\n",
      "['flushot', 'kayburley']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
      "<ipython-input-199-dc682236291b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n"
     ]
    }
   ],
   "source": [
    "global gl_hashtag\n",
    "stop_hash_word = ['COVID19']\n",
    "for i in range(6):\n",
    "    stop_hash_word = ['','covid19','covid-19','vaccine','coronavirus','covid','covid-19','china\\n\\nhttps://t.co/mg6dph4xp7','edappadipalaniswami', 'bo…']\n",
    "    selected_class = joined_df_2[joined_df_2.true_class == i+1]\n",
    "    gl_hashtag = []\n",
    "    clean_df = clean_txt(selected_class)\n",
    "    for word in stop_hash_word:\n",
    "        gl_hashtag = list(filter(lambda a: a != word, gl_hashtag))\n",
    "    print(len(df_tags['hashtag'].unique()))\n",
    "    print(len(df_tags))\n",
    "    print(len(selected_class))\n",
    "    df_tags = pd.DataFrame(gl_hashtag,columns=['hashtag'])\n",
    "    df_count = df_tags.groupby(['hashtag']).size().reset_index(name='counts')\n",
    "    print(list(df_count.sort_values(by='counts',ascending = False).head(20)['hashtag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=['going','like','think','ever','still','make','one','could','even','take','people','good','says','developed','get','may',\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "stop_hash_word = ['vaccines','year','rt','covid19','virus','via','covid-19','vaccine','coronavirus','covid','covid-19','china\\n\\nhttps://t.co/mg6dph4xp7','edappadipalaniswami', 'bo…','19', 'bo', 'china', 'co', 'https', 'mg6dph4xp7']\n",
    "stopword.extend(stop_hash_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l2',use_idf=True,max_features=500,stop_words=stopword,ngram_range=(1,2))\n",
    "tfIdf = vectorizer.fit_transform(joined_df_2['tweet_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = vectorizer.get_feature_names()\n",
    "row_keys=[]\n",
    "for i in range(412):\n",
    "    row_keys.append([vocabs[i] for i in (tfIdf[i]).nonzero()[1]])\n",
    "joined_df_2['keywords'] = row_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords = [item for sublist in list(joined_df_2['keywords']) for item in sublist]\n",
    "top_keywords = [key for key, value in Counter(class_keywords).most_common()][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keyword_each_class=[]\n",
    "for i in range(6):\n",
    "    selected_class = joined_df_2[joined_df_2.true_class == i+1]\n",
    "    class_keywords = [item for sublist in list(selected_class['keywords']) for item in sublist]\n",
    "    top_keywords = [key for key, value in Counter(class_keywords).most_common()][:20]\n",
    "    keyword_each_class.append(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['trump',\n",
       "  'office',\n",
       "  'safety',\n",
       "  'safety office',\n",
       "  'last',\n",
       "  'shut',\n",
       "  'office last',\n",
       "  'shut safety',\n",
       "  'administration',\n",
       "  'trump administration',\n",
       "  'administration shut',\n",
       "  'plan',\n",
       "  'closed',\n",
       "  'last plan',\n",
       "  'cuomo',\n",
       "  'ready',\n",
       "  'skeptical',\n",
       "  'also',\n",
       "  'gates',\n",
       "  'long'],\n",
       " ['never',\n",
       "  'flu',\n",
       "  'hiv',\n",
       "  'disease',\n",
       "  'hopes',\n",
       "  'pinning',\n",
       "  're',\n",
       "  'drug',\n",
       "  'long',\n",
       "  'around',\n",
       "  'dose realism',\n",
       "  'hopes dose',\n",
       "  'pinning hopes',\n",
       "  're pinning',\n",
       "  'dose',\n",
       "  'realism',\n",
       "  'amp',\n",
       "  'rushed',\n",
       "  'disappear',\n",
       "  'might'],\n",
       " ['trial',\n",
       "  'volunteer',\n",
       "  'astrazeneca',\n",
       "  'brazil',\n",
       "  'dies',\n",
       "  'oxford',\n",
       "  'died',\n",
       "  'astrazeneca trial',\n",
       "  'trial dies',\n",
       "  'dies brazil',\n",
       "  'death',\n",
       "  'trials',\n",
       "  'university',\n",
       "  'volunteers',\n",
       "  'volunteer astrazeneca',\n",
       "  'oxford trial',\n",
       "  'uk',\n",
       "  'volunteer oxford',\n",
       "  'trial volunteer',\n",
       "  'volunteer dies'],\n",
       " ['government',\n",
       "  'gates',\n",
       "  'amp',\n",
       "  'bill',\n",
       "  'volunteers',\n",
       "  'bill gates',\n",
       "  'us',\n",
       "  'trust',\n",
       "  'shot',\n",
       "  'called',\n",
       "  'bjp',\n",
       "  'shut',\n",
       "  'trump',\n",
       "  'exactly',\n",
       "  'citizens',\n",
       "  'ccp',\n",
       "  'chinese',\n",
       "  'pigs',\n",
       "  'human',\n",
       "  'infect'],\n",
       " ['never',\n",
       "  'let',\n",
       "  'say',\n",
       "  'way hell',\n",
       "  'children',\n",
       "  'hell',\n",
       "  'way',\n",
       "  'please let',\n",
       "  'want',\n",
       "  'please',\n",
       "  'flu shot',\n",
       "  'hard',\n",
       "  'vax',\n",
       "  'shot',\n",
       "  'anti',\n",
       "  'flu',\n",
       "  'must video',\n",
       "  'taken must',\n",
       "  'must taken',\n",
       "  'video'],\n",
       " ['flu',\n",
       "  '99',\n",
       "  'never',\n",
       "  'really',\n",
       "  'getting',\n",
       "  'something 99',\n",
       "  'something',\n",
       "  'mind',\n",
       "  're',\n",
       "  'guardian',\n",
       "  'used',\n",
       "  'hiv',\n",
       "  'let',\n",
       "  'makes',\n",
       "  'test',\n",
       "  'vaccination',\n",
       "  'mandatory',\n",
       "  'without',\n",
       "  'johnson',\n",
       "  'sure']]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_each_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(df):\n",
    "    #df[\"og_tweet_txt\"] = df['og_tweet_txt'].apply(lambda x: processing(x))\n",
    "    df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
    "    #df[df[\"tweet_txt\"].apply(is_english)]\n",
    "    #df[df[\"og_tweet_txt\"].apply(is_english)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "global gl_hashtag\n",
    "gl_hashtag = []\n",
    "clean_df = clean_txt(test_df)\n",
    "gl_hashtag = list(filter(lambda a: a != '', gl_hashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26949\n",
      "158942\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>COVID19</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25958</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>7569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>COVID</td>\n",
       "      <td>4187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18312</th>\n",
       "      <td>coronavirus</td>\n",
       "      <td>3702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>Covid19</td>\n",
       "      <td>2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>COVIDVaccine</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>Covid</td>\n",
       "      <td>1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16216</th>\n",
       "      <td>WhatsHappeningInMyanmar</td>\n",
       "      <td>1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15386</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>1694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18400</th>\n",
       "      <td>covid</td>\n",
       "      <td>1629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>CCPVirus</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>DrLiMengYan1</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14086</th>\n",
       "      <td>TakeDownTheCCP</td>\n",
       "      <td>1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26896</th>\n",
       "      <td>爆料革命</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26902</th>\n",
       "      <td>郭文贵</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>COVIDー19</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16473</th>\n",
       "      <td>YanLiMeng</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26903</th>\n",
       "      <td>闫丽梦</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18402</th>\n",
       "      <td>covid19</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       hashtag  counts\n",
       "2259                   COVID19   10625\n",
       "25958                  vaccine    7569\n",
       "2255                     COVID    4187\n",
       "18312              coronavirus    3702\n",
       "3457                   Covid19    2551\n",
       "2479              COVIDVaccine    1927\n",
       "3456                     Covid    1864\n",
       "16216  WhatsHappeningInMyanmar    1814\n",
       "15386                  Vaccine    1694\n",
       "18400                    covid    1629\n",
       "3322               Coronavirus    1515\n",
       "2134                  CCPVirus    1280\n",
       "4480              DrLiMengYan1    1265\n",
       "14086           TakeDownTheCCP    1264\n",
       "26896                     爆料革命    1262\n",
       "26902                      郭文贵    1262\n",
       "2553                  COVIDー19    1228\n",
       "16473                YanLiMeng    1150\n",
       "26903                      闫丽梦    1149\n",
       "18402                  covid19    1051"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags = pd.DataFrame(gl_hashtag,columns=['hashtag'])\n",
    "print(len(df_tags['hashtag'].unique()))\n",
    "print(len(df_tags))\n",
    "df_count = df_tags.groupby(['hashtag']).size().reset_index(name='counts')\n",
    "df_count.sort_values(by='counts',ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID19',\n",
       " 'vaccine',\n",
       " 'COVID',\n",
       " 'coronavirus',\n",
       " 'Covid19',\n",
       " 'COVIDVaccine',\n",
       " 'Covid',\n",
       " 'WhatsHappeningInMyanmar',\n",
       " 'Vaccine',\n",
       " 'covid',\n",
       " 'Coronavirus',\n",
       " 'CCPVirus',\n",
       " 'DrLiMengYan1',\n",
       " 'TakeDownTheCCP',\n",
       " '爆料革命',\n",
       " '郭文贵',\n",
       " 'COVIDー19',\n",
       " 'YanLiMeng',\n",
       " '闫丽梦',\n",
       " 'covid19',\n",
       " 'vaccines',\n",
       " 'Pfizer',\n",
       " 'COVID19Vaccine',\n",
       " 'news',\n",
       " 'auspol',\n",
       " 'CovidVaccine',\n",
       " 'Feb20Coup',\n",
       " 'Health',\n",
       " 'vaccination',\n",
       " 'Feb24Coup',\n",
       " 'BREAKING',\n",
       " 'cdnpoli',\n",
       " 'pandemic',\n",
       " 'NHS',\n",
       " 'COVAX',\n",
       " 'PeoplesVaccine',\n",
       " 'vaccinated',\n",
       " 'lockdown',\n",
       " 'SmartNews',\n",
       " 'India',\n",
       " 'AstraZeneca',\n",
       " 'Israel',\n",
       " 'cvspharmtech',\n",
       " 'WhatsHappeninglnMyanmar',\n",
       " 'pharmtech',\n",
       " 'WTO',\n",
       " 'healthcare',\n",
       " 'CrimesAgainstHumanity',\n",
       " 'Biden',\n",
       " 'News']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_count.sort_values(by='counts',ascending = False).head(50)['hashtag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = pd.read_csv(r\"C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\lstm_pred\\LSTM_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23909710916038648"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_pred.loc[lstm_pred['prediction'] == -1])/len(lstm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85471"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_pred.loc[lstm_pred['prediction'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cleaned.pkl\", 'rb') as f:\n",
    "    clean_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72879"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_retweet['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = pd.read_excel('./cutoff50.xlsx')\n",
    "lstm_pred = lstm_pred.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = lstm_pred.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retweet = lstm_pred[lstm_pred['og_tweet_id'].isna()].copy()\n",
    "retweet = lstm_pred[lstm_pred['og_tweet_id'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_pred = lstm_pred.drop('og_tweet_id',axis=1)\n",
    "prediction_joined = test_df.set_index('tweet_id').join(lstm_pred.set_index('tweet_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retweet = prediction_joined[prediction_joined['og_tweet_time'].isna()].copy()\n",
    "retweet = prediction_joined[prediction_joined['og_tweet_time'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2519198940748097\n",
      "60884\n",
      "0.19541213493071274\n",
      "14158\n"
     ]
    }
   ],
   "source": [
    "print(len(retweet.loc[retweet['prediction'] == -1])/len(retweet))\n",
    "print(len(retweet.loc[retweet['prediction'] == -1]))\n",
    "print(len(no_retweet.loc[no_retweet['prediction'] == -1])/len(no_retweet))\n",
    "print(len(no_retweet.loc[no_retweet['prediction'] == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7521583283460456\n",
      "236277\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_pred.loc[lstm_pred['prediction'] == 0])/len(lstm_pred))\n",
    "print(len(lstm_pred.loc[lstm_pred['prediction'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241680"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
