{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import cm\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Cleaning methods\n",
    "def clean_emoji(text):\n",
    "    text=emoji.demojize(text)\n",
    "    return text\n",
    "def remove_url(text):\n",
    "    text = ' '.join(re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\" \", text).split())\n",
    "    return text\n",
    "def remove_symbol(text):\n",
    "    text = str.replace(text,',',' ')\n",
    "    text = str.replace(text,'+',' ')\n",
    "    text = str.replace(text,'=',' ')\n",
    "    text = ' '.join(re.sub(\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\",\" \", text).split())\n",
    "    text = str.replace(text,\"'\",' ')\n",
    "    text = str.replace(text,'\"',' ')\n",
    "    text = str.replace(text,'!',' ')\n",
    "    text = str.replace(text,'^',' ')\n",
    "    text = str.replace(text,'(',' ')\n",
    "    text = str.replace(text,')',' ')\n",
    "    text = str.replace(text,'%',' ')\n",
    "    text = str.replace(text,'-',' ')\n",
    "    text = str.replace(text,'_',' ')\n",
    "    text = str.replace(text,'|',' ')\n",
    "    text = str.replace(text,'.',' ')\n",
    "    text = str.replace(text,':',' ')\n",
    "    return text    \n",
    "## this is the optional function we can use to remove @ and other. Might not use it if we need to analyze retweets\n",
    "def optional_rm(text):\n",
    "    all_words = text.split(' ')\n",
    "    remove_words = []\n",
    "    for each in all_words:\n",
    "        if each == ' ' or each == '':\n",
    "            continue\n",
    "        if each[0] == '#' or each[0] == '@':\n",
    "            remove_words.append(each)\n",
    "            if each[0] == '#':\n",
    "                hashtags = each.split('#')\n",
    "                for tag in hashtags:\n",
    "                    tag = re.sub(r'[^\\w\\s]','',tag)\n",
    "                    gl_hashtag.append(tag.lower())\n",
    "\n",
    "    for word in remove_words:\n",
    "        all_words.remove(word)\n",
    "    text = ' '.join(all_words)\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    new_text = []\n",
    "    for each in text:\n",
    "        if(str.find(each,'http') != -1):\n",
    "            continue\n",
    "        \n",
    "        if not each.isalnum():\n",
    "            continue\n",
    "        new_text.append(str.lower(each));\n",
    "    text = ' '.join(new_text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        if detect(text) == \"en\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        return True\n",
    "        print(e)   \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True    \n",
    "def processing(text):\n",
    "    #text=clean_emoji(text)\n",
    "    #text=remove_url(text)\n",
    "    #text = remove_symbol(text)\n",
    "    text = optional_rm(text)\n",
    "    return text\n",
    "## Import Data\n",
    "def read_json_to_dataframe(filename):\n",
    "    print(filename)\n",
    "    #f = open(filename,encoding = 'utf-16').read()\n",
    "    f1 = pd.read_json(filename, orient='records',lines=True,encoding = 'utf-16')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-19-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-20-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-21-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-22-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-23-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-24-2020.json\n",
      "C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/10-25-2020.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(371940, 23)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##read files\n",
    "def read_in(path):\n",
    "    files = os.listdir(path)\n",
    "    all_dfs = []\n",
    "    #print(files)\n",
    "    for i in files:\n",
    "        if 'json' in i:\n",
    "            each_df = read_json_to_dataframe(path+i)\n",
    "            all_dfs.append(each_df)\n",
    "            \n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "all_df = read_in('C:/Users/luoyu/Desktop/USCISI/vaccine hesitancy/raw/')\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get rid of the folded tweets\n",
    "test_df = all_df.copy()\n",
    "test_df=test_df[~test_df['og_tweet_txt'].str.endswith('\\u2026',na=True)]\n",
    "test_df=test_df[~((test_df['tweet_txt'].str.endswith('\\u2026',na=True)) & (test_df['og_tweet_txt'].str.strip() == \"\"))]\n",
    "test_df = test_df.drop_duplicates(subset=['tweet_txt'])\n",
    "# test_df=test_df.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(df):\n",
    "    #df[\"og_tweet_txt\"] = df['og_tweet_txt'].apply(lambda x: processing(x))\n",
    "    df['tweet_txt'] = df['tweet_txt'].apply(lambda x: processing(x))\n",
    "    #df[df[\"tweet_txt\"].apply(is_english)]\n",
    "    #df[df[\"og_tweet_txt\"].apply(is_english)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22939\n",
      "89188\n"
     ]
    }
   ],
   "source": [
    "global gl_hashtag\n",
    "gl_hashtag = []\n",
    "test_copy = test_df.copy()\n",
    "clean_df = clean_txt(test_copy)\n",
    "stop_hash_word = ['','covidvaccine','corona','covax','covidvaccination','covid19vaccination','coronavirus','郭文贵\\n','爆料革命','pandemic','nhs','7news','covidvaccines','郭文贵','covid19','breaking:','health','covid19\\nhttpstcoi6p9ute11f','vaccination','news','covid19\\n','covid19vaccine','vaccines,','covidー19','breaking','covid-19','vaccine','vaccines','covid__19','covid_19','covid19,','coronavirus','covid','covid-19','china\\n\\nhttps://t.co/mg6dph4xp7','edappadipalaniswami', 'bo…']\n",
    "for word in stop_hash_word:\n",
    "        gl_hashtag = list(filter(lambda a: a != word, gl_hashtag))\n",
    "df_tags = pd.DataFrame(gl_hashtag,columns=['hashtag'])\n",
    "print(len(df_tags['hashtag'].unique()))\n",
    "print(len(df_tags))\n",
    "df_count = df_tags.groupby(['hashtag']).size().reset_index(name='counts')\n",
    "top_hashtags_old = df_count.sort_values(by='counts',ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword comaprision with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./common english word 1000.txt') as f:\n",
    "    content = f.readlines()\n",
    "stopword = [x.strip() for x in content] \n",
    "with open('./stopword.txt') as f:\n",
    "    content = f.readlines()\n",
    "stopword.extend([x.strip() for x in content])\n",
    "stopword_more=['getting', 'going','like','think','ever','still','make','one','could','even','take','people','good','says','developed','get','may',\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "stopword.extend(stopword_more)\n",
    "stop_hash_word = ['weeks','county', 'didn', 'doesn', 'hasn', 'haven', 'isn', 'keeps', 'shouldn', 'tv', 'days','amp','000','ll','ve','mr', 'mrs', 'pm', 'dr', 'vaccines','year','rt','covid19','virus','via','covid-19','vaccine','coronavirus','covid','covid-19','china\\n\\nhttps://t.co/mg6dph4xp7','edappadipalaniswami', 'bo…','19', 'bo', 'china', 'co', 'https', 'mg6dph4xp7']\n",
    "stopword.extend(stop_hash_word)\n",
    "stopword = list(set(stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luoyu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['al', 'american', 'congress', 'democrat', 'republican'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l2',use_idf=True,max_features=5000,stop_words=stopword,ngram_range=(1,2))\n",
    "tfIdf = vectorizer.fit_transform(test_df['tweet_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<128472x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 560913 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128472\n",
    "vocabs = vectorizer.get_feature_names()\n",
    "row_keys=[]\n",
    "for i in range(size):\n",
    "    row_keys.append([vocabs[i] for i in (tfIdf[i]).nonzero()[1]])\n",
    "test_df['keywords'] = row_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords = [item for sublist in list(test_df['keywords']) for item in sublist]\n",
    "top_keywords = [(key,\"{0:.2f}%\".format(value/size * 100)) for key, value in Counter(class_keywords).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dict = Counter(class_keywords)\n",
    "new_data_df = pd.DataFrame(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_dict = Counter(class_keywords)\n",
    "old_data_df = pd.DataFrame(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_df = new_data_df.rename(columns={0: \"keyword\", 1: \"percentage newdata\"})\n",
    "old_data_df = old_data_df.rename(columns={0: \"keyword\", 1: \"percentage olddata\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = old_data_df.set_index('keyword').join(new_data_df.set_index('keyword'), on='keyword',how='inner')\n",
    "joined['percentage newdata'] = joined['percentage newdata'].str.rstrip('%').astype('float')/100\n",
    "joined['percentage olddata'] = joined['percentage olddata'].str.rstrip('%').astype('float')/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['difference'] = abs(joined['percentage newdata'] - joined['percentage olddata'])\n",
    "joined['old_to_new_incease'] = np.where(joined['percentage newdata']-joined['percentage olddata'] > 0,1,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sorted = joined.sort_values(by=['difference'],ascending=False)\n",
    "#joined_sorted.to_excel('./old_new_data_comparision_full_2531.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage olddata</th>\n",
       "      <th>percentage newdata</th>\n",
       "      <th>difference</th>\n",
       "      <th>old_to_new_incease</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mask</th>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masks</th>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wearing masks</th>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wearing mask</th>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masks distancing</th>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask wearing</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask distance</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masking</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  percentage olddata  percentage newdata  difference  \\\n",
       "keyword                                                                \n",
       "mask                          0.0066              0.0088      0.0022   \n",
       "masks                         0.0056              0.0062      0.0006   \n",
       "wearing masks                 0.0007              0.0009      0.0002   \n",
       "wearing mask                  0.0007              0.0011      0.0004   \n",
       "masks distancing              0.0004              0.0005      0.0001   \n",
       "mask wearing                  0.0003              0.0005      0.0002   \n",
       "mask distance                 0.0003              0.0004      0.0001   \n",
       "masking                       0.0002              0.0005      0.0003   \n",
       "Total                         0.0148              0.0189      0.0041   \n",
       "\n",
       "                  old_to_new_incease  \n",
       "keyword                               \n",
       "mask                             1.0  \n",
       "masks                            1.0  \n",
       "wearing masks                    1.0  \n",
       "wearing mask                     1.0  \n",
       "masks distancing                 1.0  \n",
       "mask wearing                     1.0  \n",
       "mask distance                    1.0  \n",
       "masking                          1.0  \n",
       "Total                            8.0  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.index = joined.index.astype('str')\n",
    "search_joined = joined[joined.index.str.contains('mask')]\n",
    "search_joined.append(search_joined.sum().rename('Total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['antivax', 'antivaxxers'], dtype='object', name='keyword')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_joined.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
