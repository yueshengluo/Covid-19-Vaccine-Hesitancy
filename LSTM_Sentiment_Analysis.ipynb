{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import gensim.downloader\n",
    "import fasttext\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import keras\n",
    "import pickle\n",
    "# import keras_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import cleaded tweet\n",
    "with open(\"./cleaned_strip@.pkl\", 'rb') as f:\n",
    "    clean_df = pickle.load(f)\n",
    "clean_df[\"combined_tweet_txt\"] = clean_df[\"tweet_txt\"] + clean_df[\"og_tweet_txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#import labeled training set\n",
    "file_list = [\"./sample_again.csv\",\"./full_tweet_sample.csv\"]\n",
    "li = []\n",
    "senti_df1 = pd.read_csv(\"./sample_again.csv\").dropna()\n",
    "senti_df1 = senti_df1[[\"full_txt\", \"sentiment\"]]\n",
    "senti_df2 = pd.read_csv(\"./full_tweet_sample.csv\").dropna()\n",
    "senti_df2 = senti_df2[[\"full_text\", \"sentiment\"]]\n",
    "senti_df2.columns = [\"full_txt\", \"sentiment\"]\n",
    "\n",
    "df = pd.concat([senti_df1,senti_df2], axis=0, ignore_index=True)\n",
    "X = df[\"full_txt\"].tolist()\n",
    "y = df[\"sentiment\"].tolist()\n",
    "\n",
    "tokens = [nltk.word_tokenize(sentences) for sentences in X]\n",
    "# list_of_txt = list(sentiment_df[\"full_text\"])\n",
    "# true_sent = list(sentiment_df[\"sentiment\"])\n",
    "# sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean stopwords for training set \n",
    "special_word = ['rt','co','amp']\n",
    "no_stop_X = []\n",
    "for each in X:\n",
    "    for word in STOPWORDS:\n",
    "        token = ' ' + word + ' '\n",
    "        each = each.replace(token, ' ')\n",
    "        for spw in special_word:\n",
    "            each = each.replace(spw, '')\n",
    "        each = each.strip()\n",
    "    no_stop_X.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean stopwords for data to label \n",
    "special_word = ['rt','co','amp']\n",
    "# no_stop_X = []\n",
    "def clean_stop(each):\n",
    "    for word in STOPWORDS:\n",
    "        token = ' ' + word + ' '\n",
    "        each = each.replace(token, ' ')\n",
    "        for spw in special_word:\n",
    "            each = each.replace(spw, '')\n",
    "        each = each.strip()\n",
    "    return each\n",
    "clean_df[\"combined_tweet_txt\"] = clean_df[\"combined_tweet_txt\"].apply(lambda x: clean_stop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "##split into train/test set\n",
    "def split_set(no_stop_X, y):\n",
    "    y_len = len(y)\n",
    "    ratio = 0.8\n",
    "    train_1_num = np.floor(y.count(1)*ratio)\n",
    "    train_1_indices = [i for i, k in enumerate(y) if k == 1]\n",
    "    train_neg_1_num = np.floor(y.count(-1)*ratio)\n",
    "    train_neg_1_indices = [i for i, k in enumerate(y) if k == -1]\n",
    "    train_0_num = np.floor(y.count(0)*ratio)\n",
    "    train_0_indices = [i for i, k in enumerate(y) if k == 0]\n",
    "    # random.selct\n",
    "    train_1_set = random.sample(train_1_indices, int(train_1_num))\n",
    "    train_neg_1_set = random.sample(train_neg_1_indices, int(train_neg_1_num))\n",
    "    train_0_set = random.sample(train_0_indices, int(train_0_num))\n",
    "    test_1_set = [i for i in train_1_indices if i not in train_1_set]\n",
    "    test_neg_1_set = [i for i in train_neg_1_indices if i not in train_neg_1_set]\n",
    "    test_0_set = [i for i in train_0_indices if i not in train_0_set]\n",
    "    train_index = train_1_set + train_neg_1_set + train_0_set\n",
    "    test_index = test_1_set + test_neg_1_set + test_0_set\n",
    "    train_x = [no_stop_X[i] for i in train_index]\n",
    "    train_y = [y[i] for i in train_index]\n",
    "    test_x = [no_stop_X[i] for i in test_index]\n",
    "    test_y = [y[i] for i in test_index]\n",
    "#     print(train_y)\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5483 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 5000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "# clean_df[\"combined_tweet_txt\"]\n",
    "tokenizer.fit_on_texts(no_stop_X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = tokenizer.texts_to_sequences(no_stop_X)\n",
    "X_total = pad_sequences(X_total, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "max_list = []\n",
    "# no_stop_X[0]\n",
    "for each in no_stop_X:\n",
    "    max_list.append(len(each.split()))\n",
    "max(max_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## tokenize train x and y:\n",
    "def tokenize(train_x,train_y, test_x, test_y, tokenizer):\n",
    "    X_train_tokenize = tokenizer.texts_to_sequences(train_x)\n",
    "    X_train_tokenize = pad_sequences(X_train_tokenize, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    X_test_tokenize = tokenizer.texts_to_sequences(test_x)\n",
    "    X_test_tokenize = pad_sequences(X_test_tokenize, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of data tensor:', X_train_tokenize.shape)\n",
    "    y_train_dummy = pd.get_dummies(train_y).values\n",
    "    y_test_dummy = pd.get_dummies(test_y).values\n",
    "    print('Shape of label tensor:', y_train_dummy.shape)\n",
    "    return X_train_tokenize, y_train_dummy, X_test_tokenize, y_test_dummy\n",
    "\n",
    "def lstm(MAX_NB_WORDS, EMBEDDING_DIM, X_train_tokenize, y_train_dummy, X_total):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_total.shape[1]))\n",
    "    model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "    model.add(tf.keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "    epochs = 6\n",
    "    batch_size = 64\n",
    "\n",
    "    history = model.fit(X_train_tokenize, y_train_dummy, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    return model\n",
    "\n",
    "def confusion(model,X_test_tokenize):\n",
    "    labels = [-1, 0, 1]\n",
    "    predictions = model.predict(X_test_tokenize)\n",
    "    final_pred = [labels[np.argmax(pred)] for pred in predictions]\n",
    "    a = confusion_matrix(test_y, final_pred, labels = labels)\n",
    "    conf = pd.DataFrame(a, index=labels, columns=labels)\n",
    "    recall = conf.loc[-1,-1]/(conf.loc[-1,-1] + conf.loc[-1,0] + conf.loc[-1,1])\n",
    "    pres = conf.loc[-1,-1]/(conf.loc[-1,-1] + conf.loc[0,-1] + conf.loc[1,-1])\n",
    "    f1 = 2*(recall*pres)/(recall+pres)\n",
    "    return recall, pres, conf, f1\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1597, 40)\n",
      "Shape of label tensor: (1597, 3)\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 2s 83ms/step - loss: 0.9315 - precision_11: 0.6780 - recall_11: 0.4015 - val_loss: 0.2454 - val_precision_11: 1.0000 - val_recall_11: 1.0000\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.8349 - precision_11: 0.6660 - recall_11: 0.6660 - val_loss: 0.3888 - val_precision_11: 1.0000 - val_recall_11: 1.0000\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.7670 - precision_11: 0.6793 - recall_11: 0.6618 - val_loss: 0.2953 - val_precision_11: 1.0000 - val_recall_11: 0.9812\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.6323 - precision_11: 0.7973 - recall_11: 0.6458 - val_loss: 0.4152 - val_precision_11: 0.9786 - val_recall_11: 0.8562\n",
      "    -1    0   1\n",
      "-1  27   59   0\n",
      " 0  18  262   0\n",
      " 1   4   31   0\n",
      "Shape of data tensor: (1597, 40)\n",
      "Shape of label tensor: (1597, 3)\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 2s 106ms/step - loss: 0.9324 - precision_12: 0.6609 - recall_12: 0.3974 - val_loss: 0.4810 - val_precision_12: 1.0000 - val_recall_12: 1.0000\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 2s 79ms/step - loss: 0.8221 - precision_12: 0.6660 - recall_12: 0.6660 - val_loss: 0.3931 - val_precision_12: 1.0000 - val_recall_12: 1.0000\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.7436 - precision_12: 0.6921 - recall_12: 0.6555 - val_loss: 0.4248 - val_precision_12: 0.9724 - val_recall_12: 0.8813\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 2s 74ms/step - loss: 0.5748 - precision_12: 0.8462 - recall_12: 0.7042 - val_loss: 0.2609 - val_precision_12: 0.9664 - val_recall_12: 0.9000\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.4346 - precision_12: 0.8843 - recall_12: 0.7975 - val_loss: 0.4003 - val_precision_12: 0.8929 - val_recall_12: 0.7812\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 2s 81ms/step - loss: 0.3152 - precision_12: 0.9156 - recall_12: 0.8608 - val_loss: 0.4263 - val_precision_12: 0.8378 - val_recall_12: 0.7750\n",
      "    -1    0   1\n",
      "-1  47   39   0\n",
      " 0  40  228  12\n",
      " 1   6   27   2\n",
      "Shape of data tensor: (1597, 40)\n",
      "Shape of label tensor: (1597, 3)\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 2s 99ms/step - loss: 0.9266 - precision_13: 0.6723 - recall_13: 0.4182 - val_loss: 0.3529 - val_precision_13: 1.0000 - val_recall_13: 1.0000\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 2s 83ms/step - loss: 0.8225 - precision_13: 0.6664 - recall_13: 0.6660 - val_loss: 0.3433 - val_precision_13: 1.0000 - val_recall_13: 1.0000\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 2s 78ms/step - loss: 0.7598 - precision_13: 0.7019 - recall_13: 0.6569 - val_loss: 0.3440 - val_precision_13: 1.0000 - val_recall_13: 0.9625\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 2s 76ms/step - loss: 0.6197 - precision_13: 0.8142 - recall_13: 0.6771 - val_loss: 0.5148 - val_precision_13: 0.8626 - val_recall_13: 0.7063\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 2s 75ms/step - loss: 0.4564 - precision_13: 0.8737 - recall_13: 0.7898 - val_loss: 0.4509 - val_precision_13: 0.8417 - val_recall_13: 0.7312\n",
      "    -1    0   1\n",
      "-1  40   46   0\n",
      " 0  41  239   0\n",
      " 1   7   28   0\n",
      "Shape of data tensor: (1597, 40)\n",
      "Shape of label tensor: (1597, 3)\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 2s 103ms/step - loss: 0.9506 - precision_14: 0.6589 - recall_14: 0.3521 - val_loss: 0.4446 - val_precision_14: 1.0000 - val_recall_14: 1.0000\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 2s 77ms/step - loss: 0.8350 - precision_14: 0.6660 - recall_14: 0.6660 - val_loss: 0.3466 - val_precision_14: 1.0000 - val_recall_14: 1.0000\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 2s 76ms/step - loss: 0.7662 - precision_14: 0.6788 - recall_14: 0.6632 - val_loss: 0.3358 - val_precision_14: 1.0000 - val_recall_14: 0.9875\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 2s 80ms/step - loss: 0.6267 - precision_14: 0.8092 - recall_14: 0.6493 - val_loss: 0.3068 - val_precision_14: 0.9867 - val_recall_14: 0.9250\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 2s 78ms/step - loss: 0.4680 - precision_14: 0.8675 - recall_14: 0.7655 - val_loss: 0.3166 - val_precision_14: 0.9728 - val_recall_14: 0.8938\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 2s 83ms/step - loss: 0.3621 - precision_14: 0.8957 - recall_14: 0.8191 - val_loss: 0.3591 - val_precision_14: 0.9041 - val_recall_14: 0.8250\n",
      "    -1    0   1\n",
      "-1  47   39   0\n",
      " 0  53  226   1\n",
      " 1   9   26   0\n",
      "Shape of data tensor: (1597, 40)\n",
      "Shape of label tensor: (1597, 3)\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 2s 96ms/step - loss: 0.9268 - precision_15: 0.6667 - recall_15: 0.4092 - val_loss: 0.4360 - val_precision_15: 1.0000 - val_recall_15: 1.0000\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 2s 74ms/step - loss: 0.8215 - precision_15: 0.6660 - recall_15: 0.6660 - val_loss: 0.4291 - val_precision_15: 1.0000 - val_recall_15: 0.9937\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 2s 81ms/step - loss: 0.7522 - precision_15: 0.6889 - recall_15: 0.6611 - val_loss: 0.4417 - val_precision_15: 0.9869 - val_recall_15: 0.9438\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.6325 - precision_15: 0.8499 - recall_15: 0.6500 - val_loss: 0.2890 - val_precision_15: 0.9745 - val_recall_15: 0.9563\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 2s 75ms/step - loss: 0.4697 - precision_15: 0.8609 - recall_15: 0.7711 - val_loss: 0.3318 - val_precision_15: 0.9603 - val_recall_15: 0.9062\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 2s 83ms/step - loss: 0.3538 - precision_15: 0.9191 - recall_15: 0.8219 - val_loss: 0.3632 - val_precision_15: 0.9267 - val_recall_15: 0.8687\n",
      "    -1    0   1\n",
      "-1  34   52   0\n",
      " 0  38  242   0\n",
      " 1   5   29   1\n"
     ]
    }
   ],
   "source": [
    "#training lstm\n",
    "from statistics import mean\n",
    "cycle = range(5)\n",
    "recall_list = []\n",
    "pres_list = []\n",
    "f1_list = []\n",
    "\n",
    "for each in cycle:\n",
    "    train_x, train_y, test_x, test_y = split_set(no_stop_X, y)\n",
    "    X_train_tokenize, y_train_dummy, X_test_tokenize, y_test_dummy = tokenize(train_x,train_y, test_x, test_y, tokenizer)\n",
    "    model = lstm(MAX_NB_WORDS, EMBEDDING_DIM, X_train_tokenize, y_train_dummy,  X_total)\n",
    "    recall, pres, conf, f1 = confusion(model,X_test_tokenize)\n",
    "    recall_list.append(recall)\n",
    "    pres_list.append(pres)\n",
    "    f1_list.append(f1)\n",
    "    print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3372093023255814,\n",
       " 0.5465116279069767,\n",
       " 0.5232558139534884,\n",
       " 0.46511627906976744,\n",
       " 0.43023255813953487]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch = 6, and -1 classification, avg pres is 0.41730363263850245 and avg recall is 0.4604651162790698\n"
     ]
    }
   ],
   "source": [
    "recall_mean = mean(recall_list)\n",
    "pres_mean = mean(pres_list)\n",
    "print(\"At epoch = 6, and -1 classification, avg pres is\",pres_mean,\"and avg recall is\",recall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 1.0260 - precision_15: 0.7258 - recall_15: 0.6534\n",
      "Test set\n",
      "  Loss: 1.026\n",
      "  Accuracy: 0.726 Recall: 0.653\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test_tokenize,y_test_dummy)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f} Recall: {:0.3f}'.format(accr[0],accr[1],accr[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classification for the whole set using LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_whole(txt_input):\n",
    "    new_complaint = [txt_input]\n",
    "    seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    pred = model.predict(padded)\n",
    "    labels = [-1, 0, 1]\n",
    "    return labels[np.argmax(pred)]\n",
    "clean_df[\"prediction\"] = clean_df[\"combined_tweet_txt\"].apply(lambda x: predict_whole(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_df = clean_df[[\"tweet_id\", \"og_tweet_id\", \"combined_tweet_txt\", \"user_loc\",\"prediction\"]]\n",
    "saving_df.to_csv ('./labled_whole_set.csv', index = False, header=True,float_format='{:f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_retweet = clean_df[clean_df['og_tweet_time'].isna()].copy()\n",
    "retweet = clean_df[clean_df['og_tweet_time'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8841226278891302"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retweet.loc[retweet['prediction'] == 1])/len(retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retweet.loc[retweet['prediction'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>og_tweet_txt</th>\n",
       "      <th>og_tweet_time</th>\n",
       "      <th>og_tweet_id</th>\n",
       "      <th>og_tweet_user_id</th>\n",
       "      <th>og_tweet_user_name</th>\n",
       "      <th>og_tweet_user_desc</th>\n",
       "      <th>og_tweet_user_vrifd</th>\n",
       "      <th>og_tweet_user_loc</th>\n",
       "      <th>tweet_txt</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>...</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet_likes</th>\n",
       "      <th>tweet_source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_acc_cr_time</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_total_tweets</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>combined_tweet_txt</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>is anyone gonna mention biden forgot he receiv...</td>\n",
       "      <td>On a Lake</td>\n",
       "      <td>...</td>\n",
       "      <td>tinberry</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-23 21:51:32</td>\n",
       "      <td>False</td>\n",
       "      <td>1737</td>\n",
       "      <td>21</td>\n",
       "      <td>is anyone gonna mention biden forgot received ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>just on presentation alone he should be put in...</td>\n",
       "      <td>Saratoga Springs, NY</td>\n",
       "      <td>...</td>\n",
       "      <td>SML531974</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td></td>\n",
       "      <td>2020-01-17 12:41:56</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>452</td>\n",
       "      <td>just presentation alone put rner</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>claire pfizer only enrolled 5 ppl over the age...</td>\n",
       "      <td>World</td>\n",
       "      <td>...</td>\n",
       "      <td>CarmenNasty2016</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td></td>\n",
       "      <td>2016-11-11 02:53:56</td>\n",
       "      <td>False</td>\n",
       "      <td>2238</td>\n",
       "      <td>39</td>\n",
       "      <td>claire pfizer enrolled 5 ppl age ppl vaccinate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>is this biden s uniqua</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>BotSatire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td></td>\n",
       "      <td>2020-08-07 03:13:40</td>\n",
       "      <td>False</td>\n",
       "      <td>1185</td>\n",
       "      <td>153</td>\n",
       "      <td>is biden uniqua</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>a coronavirus vaccine nears final tests in cub...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>newspointpn</td>\n",
       "      <td>0</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td></td>\n",
       "      <td>2020-04-03 06:34:21</td>\n",
       "      <td>False</td>\n",
       "      <td>4977</td>\n",
       "      <td>2</td>\n",
       "      <td>a ronavirus vaccine nears final tests cuba tou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970674</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>pushpin ghana covid 19 vaccine timelines</td>\n",
       "      <td>Accra</td>\n",
       "      <td>...</td>\n",
       "      <td>OnuaTV</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>OnuaMaakye</td>\n",
       "      <td>2019-07-26 10:35:15</td>\n",
       "      <td>False</td>\n",
       "      <td>7451</td>\n",
       "      <td>2569</td>\n",
       "      <td>pushpin ghana vid 19 vaccine timelines</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970682</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>you know the covid vaccine doesn t work what p...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>banthebbc</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td></td>\n",
       "      <td>2012-12-25 19:05:38</td>\n",
       "      <td>False</td>\n",
       "      <td>98877</td>\n",
       "      <td>81975</td>\n",
       "      <td>you know vid vaccine work possible reason uld ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970683</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>israeli researchers find covid vaccine working...</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>joseffederman</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td></td>\n",
       "      <td>2011-02-09 09:40:45</td>\n",
       "      <td>False</td>\n",
       "      <td>1662</td>\n",
       "      <td>2274</td>\n",
       "      <td>israeli researchers find vid vaccine working p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970687</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nana addo to take the first shot of the covid ...</td>\n",
       "      <td>Accra, Ghana</td>\n",
       "      <td>...</td>\n",
       "      <td>sabonzy</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-17 22:53:01</td>\n",
       "      <td>False</td>\n",
       "      <td>42764</td>\n",
       "      <td>16715</td>\n",
       "      <td>nana addo take first shot vid 19 vaccine broug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970701</th>\n",
       "      <td></td>\n",
       "      <td>NaT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>final answer ba sis clown face</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NMissme</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td></td>\n",
       "      <td>2012-12-19 08:20:13</td>\n",
       "      <td>False</td>\n",
       "      <td>10603</td>\n",
       "      <td>216</td>\n",
       "      <td>final answer ba sis clown face</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281128 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       og_tweet_txt og_tweet_time og_tweet_id og_tweet_user_id  \\\n",
       "3                             NaT                                \n",
       "7                             NaT                                \n",
       "9                             NaT                                \n",
       "10                            NaT                                \n",
       "18                            NaT                                \n",
       "...             ...           ...         ...              ...   \n",
       "970674                        NaT                                \n",
       "970682                        NaT                                \n",
       "970683                        NaT                                \n",
       "970687                        NaT                                \n",
       "970701                        NaT                                \n",
       "\n",
       "       og_tweet_user_name og_tweet_user_desc og_tweet_user_vrifd  \\\n",
       "3                                                                  \n",
       "7                                                                  \n",
       "9                                                                  \n",
       "10                                                                 \n",
       "18                                                                 \n",
       "...                   ...                ...                 ...   \n",
       "970674                                                             \n",
       "970682                                                             \n",
       "970683                                                             \n",
       "970687                                                             \n",
       "970701                                                             \n",
       "\n",
       "       og_tweet_user_loc                                          tweet_txt  \\\n",
       "3                         is anyone gonna mention biden forgot he receiv...   \n",
       "7                         just on presentation alone he should be put in...   \n",
       "9                         claire pfizer only enrolled 5 ppl over the age...   \n",
       "10                                                   is this biden s uniqua   \n",
       "18                        a coronavirus vaccine nears final tests in cub...   \n",
       "...                  ...                                                ...   \n",
       "970674                             pushpin ghana covid 19 vaccine timelines   \n",
       "970682                    you know the covid vaccine doesn t work what p...   \n",
       "970683                    israeli researchers find covid vaccine working...   \n",
       "970687                    nana addo to take the first shot of the covid ...   \n",
       "970701                                       final answer ba sis clown face   \n",
       "\n",
       "                    user_loc  ...        user_name tweet_likes  \\\n",
       "3                  On a Lake  ...         tinberry           0   \n",
       "7       Saratoga Springs, NY  ...        SML531974           0   \n",
       "9                      World  ...  CarmenNasty2016           0   \n",
       "10                      None  ...        BotSatire           0   \n",
       "18                      None  ...      newspointpn           0   \n",
       "...                      ...  ...              ...         ...   \n",
       "970674                 Accra  ...           OnuaTV           0   \n",
       "970682                  None  ...        banthebbc           0   \n",
       "970683                  None  ...    joseffederman           0   \n",
       "970687          Accra, Ghana  ...          sabonzy           0   \n",
       "970701                  None  ...          NMissme           0   \n",
       "\n",
       "               tweet_source    hashtags    user_acc_cr_time user_verified  \\\n",
       "3           Twitter Web App             2009-04-23 21:51:32         False   \n",
       "7       Twitter for Android             2020-01-17 12:41:56         False   \n",
       "9           Twitter Web App             2016-11-11 02:53:56         False   \n",
       "10          Twitter Web App             2020-08-07 03:13:40         False   \n",
       "18                    IFTTT             2020-04-03 06:34:21         False   \n",
       "...                     ...         ...                 ...           ...   \n",
       "970674   Twitter for iPhone  OnuaMaakye 2019-07-26 10:35:15         False   \n",
       "970682      Twitter Web App             2012-12-25 19:05:38         False   \n",
       "970683   Twitter for iPhone             2011-02-09 09:40:45         False   \n",
       "970687  Twitter for Android             2012-01-17 22:53:01         False   \n",
       "970701   Twitter for iPhone             2012-12-19 08:20:13         False   \n",
       "\n",
       "        user_total_tweets user_followers  \\\n",
       "3                    1737             21   \n",
       "7                   13152            452   \n",
       "9                    2238             39   \n",
       "10                   1185            153   \n",
       "18                   4977              2   \n",
       "...                   ...            ...   \n",
       "970674               7451           2569   \n",
       "970682              98877          81975   \n",
       "970683               1662           2274   \n",
       "970687              42764          16715   \n",
       "970701              10603            216   \n",
       "\n",
       "                                       combined_tweet_txt prediction  \n",
       "3       is anyone gonna mention biden forgot received ...          0  \n",
       "7                        just presentation alone put rner          0  \n",
       "9       claire pfizer enrolled 5 ppl age ppl vaccinate...          0  \n",
       "10                                        is biden uniqua          0  \n",
       "18      a ronavirus vaccine nears final tests cuba tou...          0  \n",
       "...                                                   ...        ...  \n",
       "970674             pushpin ghana vid 19 vaccine timelines          0  \n",
       "970682  you know vid vaccine work possible reason uld ...          0  \n",
       "970683  israeli researchers find vid vaccine working p...          0  \n",
       "970687  nana addo take first shot vid 19 vaccine broug...          0  \n",
       "970701                     final answer ba sis clown face         -1  \n",
       "\n",
       "[281128 rows x 25 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
