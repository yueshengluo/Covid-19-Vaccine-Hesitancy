{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\luoyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gensim\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import gensim.downloader\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./cleaned_strip@.pkl\", 'rb') as f:\n",
    "    clean_df = pickle.load(f)\n",
    "list_of_txt = list(clean_df[\"tweet_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec with Fasttext\n",
    "from gensim.models import FastText\n",
    "FasTxt = FastText(list_of_txt, size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(sentences) for sentences in list_of_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-74683f6d8259>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  sent_vec += model[word]*tf_idf_score\n"
     ]
    }
   ],
   "source": [
    "#sentence embedding\n",
    "def embedding(list_of_txt, model,size):\n",
    "    Tfidf_model = TfidfVectorizer()\n",
    "    Tfidf_model.fit(list_of_txt)\n",
    "    tf_idf_dict = dict(zip(Tfidf_model.get_feature_names(), list(Tfidf_model.idf_)))\n",
    "    documents = []\n",
    "    for count,tweet in enumerate(list_of_txt):\n",
    "    #word_vectors = []\n",
    "        weight_sum = 0\n",
    "        for word in tokens[count]: # or your logic for separating tokens\n",
    "            sent_vec = np.zeros(size)\n",
    "            if word in tf_idf_dict:\n",
    "                tf_idf_score = tf_idf_dict[word]\n",
    "                sent_vec += model[word]*tf_idf_score\n",
    "                weight_sum += tf_idf_score\n",
    "                #word_vectors.append(sent_vec)\n",
    "        if weight_sum != 0:\n",
    "            sent_vec /= weight_sum\n",
    "        documents.append(sent_vec)\n",
    "    return documents \n",
    "documents = embedding(list_of_txt,FasTxt,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add word2vec vectors to lstm_pred column\n",
    "clean_df['wordvec'] = documents\n",
    "\n",
    "clean_df['word_len'] = clean_df['tweet_txt'].str.len()\n",
    "clean_df = clean_df[clean_df.word_len > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import manual picked cluster center for initialization \n",
    "cluster_centers = pd.read_excel(r\"C:\\Users\\luoyu\\Desktop\\USCISI\\vaccine hesitancy\\newdata_cluster_center.xlsx\")\n",
    "cluster_centers = cluster_centers[['class','tweet_txt']]\n",
    "#join cluster_centers with lstm_pred_negative to add word vectors\n",
    "centers_joined = cluster_centers.set_index('tweet_txt').join(clean_df.set_index('tweet_txt'), how='left')\n",
    "centers_joined = centers_joined.reset_index()\n",
    "centers = list(centers_joined['wordvec'])\n",
    "\n",
    "X = list(clean_df['wordvec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.03855947e-04, -1.42354782e-04,  1.55880813e-04,  1.79567987e-04,\n",
       "         3.09365247e-04,  2.81496719e-05, -1.78963827e-05, -1.27521085e-04,\n",
       "        -1.68566515e-04,  1.33302831e-04,  2.47840000e-04, -2.27419539e-06,\n",
       "         1.28865478e-05, -7.41904598e-05, -3.19650772e-04, -2.03638431e-05,\n",
       "         1.27557952e-04, -4.09468466e-05,  1.07134458e-04, -1.69930737e-04,\n",
       "         1.28514347e-04,  1.72189524e-04,  2.32762248e-04,  6.71739908e-05,\n",
       "         3.60771643e-04]),\n",
       " array([ 3.10531109e-04,  1.15386963e-04, -9.11906315e-05,  1.28709393e-04,\n",
       "         1.40018084e-05, -1.06347971e-04, -3.48336704e-05,  1.43916033e-04,\n",
       "        -6.46919740e-05, -2.99151239e-04,  1.10456175e-05, -7.09391938e-05,\n",
       "        -1.88499540e-04, -2.01958885e-04, -1.99346910e-04,  7.16025063e-05,\n",
       "         1.87947726e-04,  1.20196190e-04, -8.99462184e-05,  2.03475927e-05,\n",
       "         6.01910568e-05, -1.56946205e-04, -4.77161928e-05, -6.75754388e-05,\n",
       "        -4.30172407e-05]),\n",
       " array([-7.23192475e-05,  6.24088441e-04, -1.26102097e-05, -5.15886376e-04,\n",
       "         5.50026231e-04,  2.18430853e-04, -4.79779175e-04, -1.52459231e-04,\n",
       "         1.96087861e-04, -3.92337944e-04,  5.79885789e-05, -6.45021312e-05,\n",
       "        -9.58486093e-04,  1.38964628e-04, -1.38179588e-03,  1.16854746e-03,\n",
       "         6.08325367e-04,  1.24250901e-03, -2.11194001e-05,  4.89725874e-04,\n",
       "         1.58092131e-03,  5.35433346e-04,  8.94822066e-04, -3.80568210e-04,\n",
       "        -1.63942381e-03]),\n",
       " array([ 5.74244316e-05, -8.59167466e-05, -1.04209761e-05,  1.05651104e-04,\n",
       "         1.23889912e-05, -1.34684807e-04, -4.49581813e-05, -1.54543103e-05,\n",
       "         1.79925588e-05,  8.27102655e-05, -2.06817676e-05,  9.95348921e-06,\n",
       "        -9.00337656e-05, -8.43227051e-05, -3.78672054e-05, -1.40453911e-05,\n",
       "        -1.62051467e-05,  5.37789493e-05, -3.14131216e-05, -5.72875924e-06,\n",
       "         4.74811624e-05,  2.12418809e-05,  1.84646163e-05, -5.98681359e-05,\n",
       "        -3.37623341e-05]),\n",
       " array([ 3.31451194e-04,  2.49249329e-04,  2.74265494e-04,  6.53728401e-05,\n",
       "        -2.29336621e-04, -1.98886209e-04, -6.70452446e-05, -9.42402559e-04,\n",
       "        -6.07142209e-04,  2.15935793e-03, -5.18579239e-04, -2.70793222e-04,\n",
       "         2.32401002e-04, -2.73522032e-04,  2.97878861e-04, -5.73687836e-04,\n",
       "        -1.27207236e-03, -1.66798758e-03, -8.78569826e-04, -7.20285648e-04,\n",
       "        -1.57201566e-03,  1.38314455e-03, -8.41469707e-05, -1.33385372e-04,\n",
       "         4.12625759e-04])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-8bc1e6cea3d0>:2: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  kmeans = KMeans(init = np.array(centers), n_clusters=5, random_state=5).fit(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 34252\n",
      "3 49561\n",
      "4 8188\n",
      "5 189077\n",
      "6 25214\n"
     ]
    }
   ],
   "source": [
    "##run kmeans with selected centers for initialization\n",
    "kmeans = KMeans(init = np.array(centers), n_clusters=5, random_state=5).fit(X)\n",
    "predict =[i+1 for i in kmeans.labels_]\n",
    "cluster_df = clean_df.copy()\n",
    "cluster_df['cluster'] = predict\n",
    "\n",
    "#print cluster size\n",
    "for n in range(5):\n",
    "    print(n+2,len([i for i in predict if i == n+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join cluster_df with all_df for cluster pre-cleand text\n",
    "cluster_df = cluster_df.reset_index()[['tweet_id','cluster','og_tweet_txt','tweet_txt']]\n",
    "#save cluster result to excel for manul verification\n",
    "cluster_df_sample = cluster_df.groupby('cluster').apply(lambda x: x.sample(100))\n",
    "cluster_df_sample.to_excel('newdata_cluster_df_sample_5x100_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileCreateError",
     "evalue": "[Errno 13] Permission denied: './newdata_sample_1000.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36m_store_workbook\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m             xlsx_file = ZipFile(self.filename, \"w\", compression=ZIP_DEFLATED,\n\u001b[0m\u001b[0;32m    630\u001b[0m                                 allowZip64=self.allow_zip64)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1249\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './newdata_sample_1000.xlsx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileCreateError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-63c7f4e841c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'og_tweet_txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tweet_txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./newdata_sample_1000.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2024\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m         )\n\u001b[1;32m-> 2026\u001b[1;33m         formatter.write(\n\u001b[0m\u001b[0;32m   2027\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2028\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    740\u001b[0m         )\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mSave\u001b[0m \u001b[0mworkbook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdisk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \"\"\"\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     def write_cells(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xlsxwriter\\workbook.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_store_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mFileCreateError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLargeZipFile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                 raise FileSizeError(\"Filesize would require ZIP64 extensions. \"\n",
      "\u001b[1;31mFileCreateError\u001b[0m: [Errno 13] Permission denied: './newdata_sample_1000.xlsx'"
     ]
    }
   ],
   "source": [
    "clean_df.sample(1000)[['tweet_id','og_tweet_txt','tweet_txt']].to_excel('./newdata_sample_1000_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = pd.read_excel('./newdata_cluster_df_sample_5x100.xlsx')\n",
    "labeled = labeled.dropna(subset=['true_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a supply chain issue and the winter storm impacting much of the country has put a damper on some covid 19 vaccine appointments', 'easier for nasa to land a rover on mars than it is for for me to get a covid vaccine syringe appointment', 'winter weather snarls delivery of covid 19 vaccine doses headed to dc area', 'historic winter storm delays covid vaccine shipments across the u s east auto news', 'rt petco park vaccine superstation closure extended due to lack of doses', 'the county just cancelled my moms first covid shot back to square one for a shot so angry 80 and get a shot this is ridiculous makes me want to cry', 'wildly unfair un says 130 countries have not received a single covid vaccine dose', 'the way we are handling the distribution of the covid vaccine is an absolute joke', 'rt north carolina vaccine providers have yet to receive nearly 300 000 19 vaccines the federal government was set', 'reading today there is a shortage of vaccines syringe expect another spike', 'astrazeneca expected to miss eu covid vaccine supply target by half in second quarter report', 'dangerous winter weather across much of the country has disrupted distribution of the covid 19 vaccine something dr says further complicates efforts to control the uk variant of the virus within u s borders', 'rt winter storms delay 6 million covid vaccine doses via', '100 000 pennsylvania residents potentially impacted by covid 19 vaccine mistake via', 'winter weather delays shipments of 6 million covid 19 vaccine doses today via', 'it s life or death non english speakers struggle to get covid 19 vaccine across us via', 'rt to get vaccinated many people are dealing with government bureaucracy for perhaps the first time schank pointed out we', 'the disabled are being systematically denied the covid vaccine across the country medically fragile people of any age are far more likely to die of covid than a healthy 65 year old every american on ssi disability or medicare regardless of age deserves the vaccine immediately', 'decentralized system forces people to scramble for appointments causing confusion frustration and mistakes states have already modeled how a one stop portal can ease vaccine sign ups and speed vaccinations time for gov hogan to make a system that works for all mders', 'new jersey covid 19 vaccine shipments may be delayed due to the winter weather conditions according to gov phil murphy']\n"
     ]
    }
   ],
   "source": [
    "print(list(labeled[labeled.true_class==7]['tweet_txt']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
